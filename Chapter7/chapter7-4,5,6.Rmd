---
title: "Chapter 7. Nonlinear Regression"
subtitle: "7.4.Smoothing Spline & 7.5.Local Regression & 7.6.Generalized Additive Models"
author: "Gayoung Moon"
date: "2025-05-15"
institute: Descendants of Lagrange \newline School of Mathematics, Statistics and Data Science \newline Sungshin Womenâ€™s University
fonttheme: "serif"
fontsize: 8pt
output:
  beamer_presentation:
    latex_engine: xelatex 
    theme: "metropolis"
header-includes:
  - \input{header_includes.tex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Smoothing Spline

## Find $f$ to minimize $L(f)$

- Given observed data $(x_1, y_1), \cdots, (x_N, y_N)$: \vt
  - We wish to obtain $f: \mathbb{R} \rightarrow \mathbb{R}$ that minimizes $L(f)$.
\vt
  - $\lambda \geq 0$ is a predetermined constant.

\vt
\vt

- $L(f)$ is defined as follows:

$$
L(f) := \sum^N_{i=1} (y_i - f(x_i))^2 + \lambda \int^{\infty}_{-\infty} \{ f''(x) \}^2dx.
$$

  

## Find $f$ to minimize $L(f)$ (Contd.)

- When $x_1< \cdots<x_N$,
\vt
  - The first term of $L(f)$ is the residual sum of squares, and the second term penalizes the complexity of the function $f$.
\vt
  - $\{ f''(x) \}^2$ intuitively expresses how non-smooth the function is at $x$.

\vt
\vt

- If $f$ is linear, the second term of $L(f)$ becomes 0. \newline So, we need to find $f$ so that the first term is minimized.


## The Form of the Model according to $\lambda$

- Since the second term of $L(f)$ is a penalty term, the shape of smoothing spline curve changes depending on the value of $\lambda$:
\vt
  - If $\lambda$ is small, the penalty is weaker, so the model has a curve that is more curved and is easier to fit to observed data.
\vt
  - If $\lambda$ is large, the penalty also increases, so the model doesn't follow the observed data well, but the curve becomes smoother and simpler.


## The Process of Optimizing Smooth Spline (1)

- First, among many functions, we can find the optimal function $f$ through a natural spline with knots $x_1, \cdots, x_N$.
\vt
\vt

- We define:
\vt
  - $f(x)$: an arbitrary function that minimizes $L(f)$,
\vt
  - $g(x)$: the natural spline with knots $x_1, \cdots, x_N$,
\vt
  - $r(x) := f(x) - g(x)$.
\vt
\vt

- Since the dimension of $g(x)$ is $N$, we can determine the coefficients $\gamma_1, \cdots, \gamma_N$ of the basis functions $h_1(x), \cdots, h_N(x)$ in $g(x)= \sum^N_{i=1} \gamma_i h_i(x)$:

$$
g(x_1)= f(x_1), \cdots, g(x_N)= f(x_N).
$$


## The Process of Optimizing Smooth Spline (1) (Contd.)

- And we can solve the following linear equation:

$$
\sum^N_{i=1} h_i(x) \, \gamma_i = f(x).
$$
\vt

- Then, note that:
\vt
  - $r(x_1)= \cdots = r_N(x_N)=0$.
\vt
  - $g(x)$ is a line for $x \leq x_1$, $x_N \leq x$ and a cubic polynomial for $x_1 < x < x_N$, respectively, which means $g'''(x)$ is a constant $\gamma_i$ for each interval $[x_i, x_{i+1}]$, specifically, $g''(x_1)= g''(x_N)=0$.
  

## The Process of Optimizing Smooth Spline (1) (Contd.)

- Then, we have

$$
\begin{aligned}
\int^{x_N}_{x_1} g''(x) r''(x) dx &= [g''(x) r'(x)]^{x_N}_{x_1} - \int^{x_N}_{x_1} g'''(x) r'(x) dx \\
&= \{ g''(x_N)r'(x_N) - g''(x_1)r'(x_1) \} - \int^{x_N}_{x_1} g'''(x) r'(x) dx \\
&= - \int^{x_N}_{x_1} g'''(x) r'(x) dx \\
&= -\sum^{N-1}_{i=1} \gamma_i \{ r(x_N)-r(x_1) \} =0.
\end{aligned}
$$


## The Process of Optimizing Smooth Spline (1) (Contd.)

- So, we have

$$
\begin{aligned}
\int^{\infty}_{-\infty}\{ f''(x) \}^2 dx &\geq \int^{x_N}_{x_1}\{ g''(x) + r''(x) \}^2 dx \\
&= \int^{x_N}_{x_1}\{ g''(x) \}^2 + \int^{x_N}_{x_1}\{ r''(x) \}^2 + 2\int^{x_N}_{x_1} g''(x) r''(x) dx \\
&= \int^{x_N}_{x_1}\{ g''(x) \}^2 + \int^{x_N}_{x_1}\{ r''(x) \}^2 dx \\
& \geq \int^{x_N}_{x_1}\{ g''(x) \}^2 dx.
\end{aligned}
$$


## The Process of Optimizing Smooth Spline (2)

- Next, we can find $f$ by finding the coefficients $\gamma_1, \cdots, \gamma_N$ of such a natural spline $f(x)= \sum^N_{i=1} \gamma_i h_i(x)$.

\vt
\vt

- Let $G= (g_{i, j})$ be the matrix with elements 

$$
g_{i, j} := \int^\infty_{-\infty} h_i''(x) h_j''(x) dx.
$$

\vt
\vt

- Here, $f(x)$ can be expressed as $X\gamma$ using the design matrix $X$, whose elements are $X_{ij}= h_j(x_i)$, and $G$ is the curvature matrix.


## The Process of Optimizing Smooth Spline (3)

- Before looking at the second term of the $L(f)$ function, let's look at the integration process of $\{ f''(x) \}^2$:

$$
\begin{aligned}
\int^\infty_{-\infty} \{ f''(x) \}^2 \, dx &= \int^\infty_{-\infty} \Big\{ \sum^N_{i=1} \gamma_i h''_i(x) \Big\}^2 \, dx \\
&= \int^\infty_{-\infty} \sum^N_{i=1} \sum^N_{j=1} \gamma_i \gamma_j h''_i(x) h''_j(x) \, dx \\
&= \sum^N_{i=1} \sum^N_{j=1} \gamma_i \gamma_j \int^\infty_{-\infty} h''_i(x) h''_j(x) \, dx.
\end{aligned}
$$



## The Process of Optimizing Smooth Spline (3) (Contd.)

- Then, the second term in $L(f)$ becomes:

$$
\begin{aligned}
\lambda \int^\infty_{-\infty} \{ f''(x) \}^2dx &= \lambda \int^\infty_{-\infty} \sum^N_{i=1} \gamma_i h_i''(x) \sum^N_{j=1} \gamma_j h_j''(x) dx \\
&= \lambda \sum^N_{i=1} \sum^N_{j=1} \gamma_i \gamma_j \int^\infty_{-\infty} h_i''(x) h_j''(x)dx \\
&= \lambda \sum^N_{i=1} \sum^N_{j=1} \gamma_i \gamma_j G_{ij} \\
&= \lambda \gamma^{\top} G \gamma. 
\end{aligned}
$$

- Thus, $L(f)$ becomes

$$
\begin{aligned}
L(f) &= \sum^N_{i=1} (y_i - f(x_i))^2 + \lambda \gamma^{\top} G \gamma \\
&= \Vert y - X\gamma \Vert^2 + \lambda \gamma^{\top} G \gamma.
\end{aligned}
$$


## The Process of Optimizing Smooth Spline (3) (Contd.)

- Since we want to minimize $L(f)$, we differentiate $L(f)$ with respect to $\gamma$,

$$
\begin{aligned}
\frac{\partial L(f)}{\partial \gamma} &= \frac{\partial}{\partial \gamma} (\Vert y - X\gamma \Vert^2 + \lambda \gamma^{\top} G \gamma) \\
&= \frac{\partial}{\partial \gamma} \{ (y-X\gamma)^{\top} (y-X\gamma) + \lambda \gamma^{\top} G \gamma \} \\
&= -2X^{\top}(y- X\gamma) + 2 \lambda G \gamma = 0.
\end{aligned}
$$

- Then:

$$
\begin{aligned}
&\Rightarrow -2X^{\top}(y- X\gamma) + 2 \lambda G \gamma = 0 \\
&\Rightarrow -X^{\top}(y-X\gamma) +  \lambda G \gamma = 0 \\
&\Rightarrow X^{\top}(y-X\gamma) = \lambda G \gamma \\
&\Rightarrow X^{\top}y = X^{\top}X\gamma + \lambda G \gamma.
\end{aligned}
$$

- If we solve the following equation for $\gamma$, we get the $\hat{\gamma}$.

$$
\hat{\gamma}= (X^{\top}X + \lambda G)^{-1}X^{\top}y.
$$


## [Example 57] Smoothing Spline for Multiple $\lambda$ Values

- By means of the following procedure, we can obtain the matrix $G$ from the knots $x_1< \cdots <x_N$.

```{r ex57-1, echo= TRUE, eval= TRUE}
G= function(x){
  n= length(x); g= matrix(0, nrow= n, ncol= n)
  for(i in 3:(n)) for(j in i:n){
    g[i, j]= 12*(x[n]-x[n-1])*(x[n-1]-x[j-2])*(x[n-1]-x[i-2])/(x[n]-x[i-2])/
      (x[n]-x[j-2])+(12*x[n-1]+6*x[j-2]-18*x[i-2])*(x[n-1]-x[j-2])^2/
      (x[n]-x[i-2])/(x[n]-x[j-2])
    g[j, i]= g[i, j]
  }
  return(g)
}
```


## [Example 57] Smoothing Spline for Multiple $\lambda$ Values

- Computing the matrix $G$ and $\hat{\gamma}$ for each $\lambda$, we draw the smoothing spline curve.

- We observe that the larger $lambda$ is, the smoother the curve.

```{r}
d= function(j, x, knots){
  K= length(knots)
  (max((x-knots[j])^3, 0)-max((x-knots[K])^3, 0))/(knots[K]-knots[j])
}

h= function(j, x, knots){
  K= length(knots)
  if (j==1) return(1)
  else if (j==2) return(x)
  else return(d(j-2, x, knots)-d(K-1, x, knots))
}

n= 100; x= runif(n, -5, 5); y=x+sin(x)*2+rnorm(n)
index= order(x); x= x[index]; y= y[index]; knots= x
X= matrix(nrow= n, ncol= n); X[, 1]= 1
for (j in 2:n) for (i in 1:n){
  X[i, j]= h(j, x[i], x)
}
G= matrix(0, n, n) # G= matrix(0, n, n)
for (i in 3:n){
  G[i, i]= 1
}

lambda.set= c(40, 400, 1000); col.set= c('red', 'blue', 'green')
for (i in 1:3){
  lambda= lambda.set[i]
  gamma= solve(t(X)%*%X + lambda*G)%*%t(X)%*%y
  g= function(u){
    S= gamma[1]; for (j in 2:n){
      S= S+gamma[j]*h(j, u, x)
    }
      return(S)
  }
  u.seq= seq(-8, 8, 0.02); v.seq= NULL
  for (u in u.seq){
    v.seq= c(v.seq, g(u))
  }
  plot(u.seq, v.seq, type= 'l', yaxt= 'n', xlab= 'x', ylab= 'g(x)', ylim= c(-8, 8), col= col.set[i])
  par(new= TRUE)
}
points(x, y); legend('topleft', paste0('lambda=', lambda.set), col= col.set, lty= 1)
title('Smoothing Spline (N=100)')
```


## Determination the Value of $\lambda$ by Corss-Validation

- In nonlinear regression, we must compute the inverse of a matrix size $N \times N$, so we need an approximation because the computation is complex for large $N$.

\vt
\vt

- However, if $N$ is not large, the value of $\lambda$ can be determined by cross-validation.

\vt
\vt

- Thus, the predictive error of CV is given by

$$
CV[\lambda] := \sum_S \Vert(I - H_S[\lambda])^{-1} e_S \Vert^2.
$$

- $H_S[\lambda]$ is defined as follows:

$$
H_S[\lambda]:= X_S(X^{\top}X+\lambda G)^{-1}X_S^{\top}.
$$


# Local Regression

## Kernel Functions in Local Regression

- Let $X$ be a set and we call a function $k: X \times X \rightarrow \mathbb{R}$ a kernel if:
\vt
  - For any $n \geq 1$ and $x_1, \cdots, x_N$, the matrix $K \in X^{n \times n}$ with $K_{i, j}= k(x_i, x_j)$ is non-negative definite.
\vt
  - For any $x, y \in X$, $k(x, y)= k(y, x)$(symmetry).

\vt
\vt

- Kernels are used to express the similarity of two elements in set $X$:
\vt
  - The more closer the $x, y \in X$ are.
\vt
  - The larger the $k(x, y)$.


## [Example 59] *Epanechnikov Kernel*

- Epanechnikov kernel is defined as:

$$
K_{\lambda}(x, y) = D\Bigg(\frac{\vert x- y\vert}{\lambda}\Bigg)
$$

$$
D(t)= \begin{cases}
\frac{3}{4}(1-t^2), \; \vert t \vert \leq 1 \\
0, \; \text{Otherwise}.
\end{cases}
$$


## [Example 59] *Epanechnikov Kernel* (Contd.)

- Example with $\lambda= 2$ and $x= \{ -1, 0, 1 \}$, then the kernel matrix becomes:

$$
\begin{bmatrix}
K_{\lambda}(x_1, y_1) & K_{\lambda}(x_1, y_2) & K_{\lambda}(x_1, y_3) \\
K_{\lambda}(x_2, y_1) & K_{\lambda}(x_2, y_2) & K_{\lambda}(x_2, y_3) \\
K_{\lambda}(x_3, y_1) & K_{\lambda}(x_3, y_2) & K_{\lambda}(x_3, y_3)
\end{bmatrix}
= \begin{bmatrix}
\frac{3}{4} & \frac{9}{16} & 0 \\
\frac{9}{16} & \frac{3}{4} & \frac{9}{16} \\
0 & \frac{9}{16} & \frac{3}{4}
\end{bmatrix}.
$$

\vt

- Its determinant is $-(3^3)/(2^9)$, small than 0.
\vt
  - Since the determinant is equal to the product of the eigenvalues, at least one of the three eigenvalues should be negative.
  

## [Example 59] *Epanechnikov Kernel* (Contd.)

- Nadaraya-Watson estimator is defined as:

$$
\hat{f}(x)= \frac{\sum^N_{i=1}K(x, x_i)y_i}{\sum^N_{j=1}K(x, x_j)}.
$$

\vt
\vt

- Then, given a new data point $x_* \in X$, the estimator returns $\hat{f}(x_*)$, which weights $y_1, \cdots, y_N$ according to the ratio

$$
\frac{K(x_*, x_1)}{\sum^N_{j=1}K(x_*, x_j)}, \cdots, \frac{K(x_*, x_N)}{\sum^N_{j=1}K(x_*, x_j)}.
$$

\vt

- Since we assume that $k(u, v)$ expresses the similarity between $u, v \in X$, the more closer $x_*$ and $x_i$ are, the larger the weight on $y_i$.


## Local Linear Regression

- In standard linear regression, we obtain $\beta \in \mathbb{R}^{p+1}$ that minimizes:

$$
\sum^N_{i=1}(y_i - [1, x_i]\beta)^2.
$$

\vt

- In local linear regression, we obtain $\beta(x) \in \mathbb{R}^{p+1}$ that minimizes:

$$
\sum^N_{i=1} k(x, x_i)(y_i - [1, x_i] \beta(x))^2.
$$
\newline for each $x \in \mathbb{R}$, where $k$ is a kernel.

\vt
- Note that $\beta(x)$ depends on $x \in \mathbb{R}^p$, which is the main difference from standard local regression.


## Weighted Least Squares Formulation

- Define:
\vt
  - $X$: matrix with rows $[1, x_i]$,
\vt
  - $W$: diagonal matrix with entries $k(x, x_i)$,
\vt
  - $y$: response vector.

\vt
\vt

- Then the loss function in local linear regression becomes:

$$
(y-X\beta(x))^{\top} W (y-X\beta(x)).
$$


## Solving for $\hat{\beta}(x)$

- Differentiate the loss function with respect to $\beta$ and set to zero:

$$
-2X^{\top}W(y-X\beta(x)) = 0.
$$

- Then, solve for $\beta(x)$:

$$
\begin{aligned}
&\Rightarrow -2X^{\top}Wy + 2X^{\top}WX\beta(x)= 0 \\
&\Rightarrow -X^{\top}Wy + X^{\top}WX\beta(x)= 0 \\
&\Rightarrow X^{\top}WX\beta(x) = X^{\top}Wy \\
&\Rightarrow \hat{\beta}(x) = (X^{\top}WX)^{-1} X^{\top}Wy \\
\end{aligned}
$$


## The Difference of Polynomial and Local Regression

- We can describe the global structure across all data with polynomial regression, and express locally existing non-linearities or irregular vibrations with local linear regression. 


```{r, out.width= '80%', fig.align= 'center'}
D=function(t) max(0.75*(1-t^2),0)
K=function(x,y,lambda) D(abs(x-y)/lambda)

local=function(x,y,z=x){
X=cbind(rep(1,n),x); yy=NULL; beta.hat=array(dim=2)
for(u in z){
w=array(dim=n); for(i in 1:n)w[i]=K(x[i],u,lambda=1); W=diag(w)
beta.hat= solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%y; yy=c(yy,beta.hat[1]+beta.hat
[2]*u)
  }
  return(yy)
}

poly=function(x,y,z=x){
n=length(x);m=length(z); X=cbind(rep(1,n),x,x^2,x^3); yy=array(dim=n);
beta.hat=array(dim=4); beta.hat= solve(t(X)%*%X)%*%t(X)%*%y;
X=cbind(rep(1,m),z,z^2,z^3); yy= X%*% beta.hat
return(yy)
}

n=30; x=runif(n)*2*pi-pi; y=sin(x)+rnorm(1)
y.1=0; y.2=0; for(k in 1:10){y.1=poly(x,y-y.2); y.2= local(x,y-y.1)}
z=seq(-2,2,0.1); par(mfrow=c(1,2))
plot(z,poly(x,y.1,z),type="l", xlab="x", ylab="f(x)", main="Polynomial Regression
 (cubic)",col="blue")
plot(z,local(x,y.2,z),type="l", xlab="x", ylab="f(x)", main="Local Linear Regression",col="blue")
```


# Generalized Additive Models (GAMs)

## Generalized Additive Models (GAMs)

- Generalized Additive Models(GAMs) are a type of statistical model that extend **Generalized Linear Models (GLMs)** by allowing the linear predictor to be a sum of smooth functions of the predictor variables.

\vt
\vt

- This flexibility makes GAMs highly effective for capturing non-linear relationships between predictors and the response variable.


## [Example 62] GAMs with the Polynomials and the Natural Spline

- The basis of the polynomials of order $p=4$ contains five functions $1, x, x^2, x^3, x^4$, and the basis of the natural spline curves with $K= 5$ knots contains $1, x, h_1(x), h_2(x), h_3(x)$.

\vt
\vt

- However, if we mix them, we obtain eight linearly independent functions.

\vt
\vt

- So, we can estimate a function $f(x)$ that can be expressed by the sum of an order $p=4$ polynomial and a $K= 5$ knot natural spline function:

$$
\hat{f}(x) = \sum^4_{j=0} \hat{\beta}_jx^j + \sum^7_{j=5} \hat{\beta}_j h_{j-2}(x).
$$

- And $\hat{\beta}= (X^{\top}X)^{-1}X^{\top}y= [\hat{\beta_0}, \cdots, \hat{\beta_\gamma} ] ^{\top}$ from observed data $(x_1, y_1), \cdots, (x_N, y_N)$, where $X= [1 \; x \; x^2 \; x^3 \; x^4 \; h_3(x) \; h_4(x) \; h_5(x)]$.

## Back-fitting in GAMs

- As for the smoothing spline curves with large sample size $N$, computing the inverse matrix is difficult.

\vt
\vt

- Also, in some case, such as local regression, the curve can't be expressed by a finite number of basis functions.

\vt
\vt

- So, we often use a technique called **back-fitting**.


## Back-fitting in GAMs (Contd.)

- Suppose that we express a function $f(x)$ as the sum of functions $f_1(x), \cdots, f_p(x)$.

\vt
\vt

- It works in three main processes:
\vt
  - First, we set $f_1(x)=f_2(x)=\cdots=f_p(x)=0$.
\vt
  - For each function $f(x)$, compute the residual $r_j(x)= f(x) - \sum_{k \neq j}f_k(x)$ and fit $f_j(x)$ to this residual.
\vt
  - Then, repeat the cycle until convergence.


## Q & A

\begin{center}
  {\bf {\Huge Q \& A}}
\end{center}



## 

\begin{center}
  {\bf {\Huge Thank you :)}}
\end{center}